{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from importlib import reload\n",
    "from jupyterthemes import jtplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch import distributions, nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# Custom imports\n",
    "from a3c import A3CVectorized\n",
    "import graph_nns as gnn\n",
    "import vectorized_env as ve\n",
    "import vectorized_agents as va\n",
    "\n",
    "%matplotlib inline\n",
    "jtplot.style()\n",
    "\n",
    "DEVICE = torch.device('cuda')\n",
    "OBS_NORM = 100. / 1999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: load a saved model state_dict from a serialized string file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('rl_agents/ss_temp.txt', 'r') as f:\n",
    "with open('runs/v4/cp_162.txt', 'r') as f:\n",
    "    serialized_string = f.readline()[2:-1].encode()\n",
    "state_dict_bytes = base64.b64decode(serialized_string)\n",
    "loaded_state_dicts = pickle.loads(state_dict_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL training\n",
    "#### TODO:\n",
    "Implement \"fictitious self-play\" to improve trained model robustness. On a related note, maybe compare trained agents against many/all agents from this notebook: https://www.kaggle.com/jamesmcguigan/santa-2020-agents-comparison\n",
    "\n",
    "Implement new RL algorithm - AWAC, and use top team's logs as training data. See: https://www.kaggle.com/masatomatsui/santa-episode-scraper\n",
    "\n",
    "Eventually, experiment with reward_type = END_OF_GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using TEMP exp_folder\n"
     ]
    }
   ],
   "source": [
    "use_rnn = True\n",
    "\n",
    "graph_nn_kwargs = dict(\n",
    "    in_features=3,\n",
    "    n_nodes=100,\n",
    "    n_hidden_layers=3,\n",
    "    layer_sizes=16,\n",
    "    layer_class=gnn.SmallRecurrentGNNLayer if use_rnn else gnn.SmallFullyConnectedGNNLayer,\n",
    "    skip_connection_n=1\n",
    ")\n",
    "model = gnn.GraphNNA3C(**graph_nn_kwargs)\n",
    "try:\n",
    "    sd = loaded_state_dicts['model_state_dict']\n",
    "    model.load_state_dict(sd)\n",
    "    print('Successfully loaded saved model')\n",
    "    del loaded_state_dicts\n",
    "except NameError:\n",
    "    pass\n",
    "except RuntimeError:\n",
    "    print('WARNING: Failed to load saved model')\n",
    "model.to(device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "env_kwargs = dict(\n",
    "    n_envs=512,\n",
    "    env_device=DEVICE,\n",
    "    out_device=DEVICE,\n",
    "    normalize_reward=False,\n",
    "    reward_type=ve.EVERY_STEP_EV,\n",
    "    obs_type=ve.LAST_STEP_OBS if use_rnn else ve.SUMMED_OBS,\n",
    "    opponent_obs_type=ve.SUMMED_OBS\n",
    ")\n",
    "rl_alg_kwargs = dict(\n",
    "    batch_size=30 if use_rnn else 30,\n",
    "    gamma=0.99\n",
    ")\n",
    "\n",
    "# Used for va.run_vectorized_vs\n",
    "def model_wrapped(states):\n",
    "    return model.sample_action(states.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "def model_constructor():\n",
    "    return gnn.GraphNNA3C(**graph_nn_kwargs)\n",
    "\n",
    "# initial_opponent_pool = []\n",
    "initial_opponent_pool = [\n",
    "    #va.BasicThompsonSampling(OBS_NORM),\n",
    "    #va.PullVegasSlotMachines(OBS_NORM),\n",
    "    #va.SavedRLAgent('a3c_agent_v0', device=DEVICE),\n",
    "    #va.SavedRLAgent('a3c_agent_v1', device=DEVICE),\n",
    "    #va.SavedRLAgent('a3c_agent_v2', device=DEVICE),\n",
    "    va.SavedRLAgent('a3c_agent_v3', device=DEVICE),\n",
    "]\n",
    "\n",
    "a3c_alg = A3CVectorized(model_constructor, optimizer, model=model, device=DEVICE,\n",
    "                        exp_folder=Path('runs/TEMP'),\n",
    "                        recurrent_model=use_rnn,\n",
    "                        clip_grads=10.,\n",
    "                        play_against_past_selves=False,\n",
    "                        n_past_selves=4,\n",
    "                        checkpoint_freq=10,\n",
    "                        initial_opponent_pool=initial_opponent_pool,\n",
    "                        opp_posterior_decay=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [55:25<00:00, 16.63s/it]\n"
     ]
    }
   ],
   "source": [
    "env_kwargs['n_envs'] = 512\n",
    "env_kwargs['opponent'] = va.SavedRLAgent('a3c_agent_v2', device=DEVICE)\n",
    "a3c_alg.train(n_episodes=200, **rl_alg_kwargs, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_reward_sums += a3c_alg.train(n_episodes=400, **rl_alg_kwargs, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = a3c_alg.model_constructor()\n",
    "temp_model.load_state_dict(a3c_alg.checkpoints[78])\n",
    "temp_model.to(device=DEVICE)\n",
    "temp_model.eval();\n",
    "\n",
    "def model_wrapped(states):\n",
    "    return temp_model.sample_action(states.unsqueeze(0)).squeeze(0)\n",
    "    #return a3c_alg.model.sample_action(states.unsqueeze(0)).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a3c_alg.opp_a / (a3c_alg.opp_a + a3c_alg.opp_b)).argmax() - len(initial_opponent_pool)\n",
    "est_opp_skill = a3c_alg.opp_a / (a3c_alg.opp_a + a3c_alg.opp_b)\n",
    "ranking = np.argsort(-est_opp_skill)\n",
    "idx = 2\n",
    "print(f'Opponent {ranking[idx]}, ranked {idx+1}, has an estimated win % of {est_opp_skill[ranking[idx]]:.3f}.')\n",
    "print(f'Checkpoint #{ranking[idx] - len(a3c_alg.initial_opponent_pool)}, path: {a3c_alg.exp_folder}/{(ranking[idx] - len(a3c_alg.initial_opponent_pool)) * a3c_alg.checkpoint_freq}_cp.txt')\n",
    "#np.argsort(-est_opp_skill)\n",
    "est_opp_skill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark the model against a previous RL agent, and various hand-crafted algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3c_alg.model.reset_hidden_states()\n",
    "#opp = va.SavedRLAgent('a3c_agent_v2')\n",
    "#opp = va.BasicThompsonSampling(OBS_NORM)\n",
    "opp = va.PullVegasSlotMachines(OBS_NORM)\n",
    "va.run_vectorized_vs(model_wrapped, opp, 'new_model', opp.name, n_envs=1000, out_device=DEVICE, obs_type=ve.LAST_STEP_OBS, opponent_obs_type=ve.SUMMED_OBS)\n",
    "a3c_alg.model.reset_hidden_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va.run_vectorized_vs(va.RandomAgent(), va.SavedRLAgent('a3c_agent_v0'), '1', '2', n_envs=1000, out_device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = ve.KaggleMABEnvTorchVectorized(**env_kwargs)\n",
    "a3c_alg.model.reset_hidden_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, _, _, _ = test_env.reset()\n",
    "a = a3c_alg.model.sample_action(s.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3c_alg.model.get_hidden_states()[0][0][0][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "va.run_vectorized_vs(model_wrapped, va.SavedRLAgent('a3c_agent_v3'), 'new_model', 'a3c_agent_v3', n_envs=4000, out_device=DEVICE)\n",
    "va.run_vectorized_vs(model_wrapped, va.SavedRLAgent('a3c_agent_v4-162'), 'new_model', 'a3c_agent_v4-162', n_envs=4000, out_device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "benchmark_env_kwargs = dict(\n",
    "    n_envs=1000,\n",
    "    out_device=DEVICE\n",
    ")\n",
    "\n",
    "## Against itself\n",
    "va.run_vectorized_vs(model_wrapped, model_wrapped, 'new_model', 'new_model', **benchmark_env_kwargs)\n",
    "#model_wrapped.reset_hidden_states()\n",
    "\n",
    "## Previous best models\n",
    "va.run_vectorized_vs(model_wrapped, va.SavedRLAgent('a3c_agent_v0'), 'new_model', 'a3c_agent_v0', **benchmark_env_kwargs)\n",
    "va.run_vectorized_vs(model_wrapped, va.SavedRLAgent('a3c_agent_v1'), 'new_model', 'a3c_agent_v1', **benchmark_env_kwargs)\n",
    "va.run_vectorized_vs(model_wrapped, va.SavedRLAgent('a3c_agent_v2'), 'new_model', 'a3c_agent_v2', **benchmark_env_kwargs)\n",
    "va.run_vectorized_vs(model_wrapped, va.SavedRLAgent('a3c_agent_v3'), 'new_model', 'a3c_agent_v3', n_envs=4000, out_device=DEVICE)\n",
    "va.run_vectorized_vs(model_wrapped, va.SavedRLAgent('a3c_agent_v4-162'), 'new_model', 'a3c_agent_v4-162', n_envs=4000, out_device=DEVICE)\n",
    "\n",
    "## Always 0 agent\n",
    "#va.run_vectorized_vs(model_wrapped, va.AlwaysFirstAgent(), 'new_model', 'always_first', **benchmark_env_kwargs)\n",
    "\n",
    "## Random agent\n",
    "#va.run_vectorized_vs(model_wrapped, va.RandomAgent(), 'new_model', 'random_agent', **benchmark_env_kwargs)\n",
    "\n",
    "## Basic thompson sampling\n",
    "va.run_vectorized_vs(model_wrapped, va.BasicThompsonSampling(OBS_NORM), 'new_model', 'basic_thompson_sampling', **benchmark_env_kwargs)\n",
    "\n",
    "## Pull vegas slot machines\n",
    "va.run_vectorized_vs(model_wrapped, va.PullVegasSlotMachines(OBS_NORM), 'new_model', 'pull_vegas_slot_machines', **benchmark_env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sd_checkpoint = model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model parameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_layer_name = []\n",
    "layer_name = []\n",
    "component_name = []\n",
    "weight_or_bias = []\n",
    "param_val = []\n",
    "for key, val in model.state_dict().items():\n",
    "    if 'message_passing_mat' in key:\n",
    "        pass\n",
    "    else:\n",
    "        for v in val.cpu().numpy().ravel():\n",
    "            param_val.append(v)\n",
    "            full_layer_name.append(key)\n",
    "            key_split = key.split('.')\n",
    "            if len(key_split) == 3:\n",
    "                ln, pn, wob = key_split\n",
    "            elif len(key_split) >= 4:\n",
    "                ln = '.'.join(key_split[0:-2])\n",
    "                pn, wob = key_split[-2:]\n",
    "            else:\n",
    "                raise RuntimeError(f'Unrecognized layer key: {key}')\n",
    "            layer_name.append(ln)\n",
    "            component_name.append(pn)\n",
    "            weight_or_bias.append(wob)\n",
    "params_df = pd.DataFrame(dict(\n",
    "    full_layer_name = full_layer_name,\n",
    "    layer_name = layer_name,\n",
    "    component_name = component_name,\n",
    "    weight_or_bias = weight_or_bias,\n",
    "    param_val = param_val\n",
    "))\n",
    "\n",
    "sns.displot(\n",
    "    params_df, x='param_val', col='weight_or_bias', row='layer_name',\n",
    "    binwidth=0.1,\n",
    "    #height=3,\n",
    "    facet_kws=dict(margin_titles=True, sharex=False, sharey=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_layer_name = []\n",
    "layer_name = []\n",
    "component_name = []\n",
    "weight_or_bias = []\n",
    "param_val = []\n",
    "for key, val in gnn.GraphNNA3C(**graph_nn_kwargs).state_dict().items():\n",
    "    if 'message_passing_mat' in key:\n",
    "        pass\n",
    "    else:\n",
    "        for v in val.cpu().numpy().ravel():\n",
    "            param_val.append(v)\n",
    "            full_layer_name.append(key)\n",
    "            key_split = key.split('.')\n",
    "            if len(key_split) == 3:\n",
    "                ln, pn, wob = key_split\n",
    "            elif len(key_split) >= 4:\n",
    "                ln = '.'.join(key_split[0:-2])\n",
    "                pn, wob = key_split[-2:]\n",
    "            else:\n",
    "                raise RuntimeError(f'Unrecognized layer key: {key}')\n",
    "            layer_name.append(ln)\n",
    "            component_name.append(pn)\n",
    "            weight_or_bias.append(wob)\n",
    "params_df = pd.DataFrame(dict(\n",
    "    full_layer_name = full_layer_name,\n",
    "    layer_name = layer_name,\n",
    "    component_name = component_name,\n",
    "    weight_or_bias = weight_or_bias,\n",
    "    param_val = param_val\n",
    "))\n",
    "\n",
    "sns.displot(\n",
    "    params_df, x='param_val', col='component_name', row='layer_name',\n",
    "    binwidth=0.1,\n",
    "    #height=3,\n",
    "    facet_kws=dict(margin_titles=True, sharex=False, sharey=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model state_dict as a serialized string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "state_dict_bytes = pickle.dumps({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    #'optimizer_state_dict': optimizer.state_dict()\n",
    "})\n",
    "serialized_string = base64.b64encode(state_dict_bytes)\n",
    "with open('rl_agents/ss_temp.txt', 'w') as f:\n",
    "    f.write(str(serialized_string))\n",
    "model.to(device=DEVICE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "model.reset_hidden_states()\n",
    "test_env = ve.KaggleMABEnvTorchVectorized(n_players=2, decay_rate=0.97, obs_type=ve.LAST_STEP_OBS)\n",
    "with torch.no_grad():\n",
    "    s, _, _, _ = test_env.reset()\n",
    "    # Take n good actions\n",
    "    for i in range(1):\n",
    "        actions = test_env.thresholds.argmax(axis=-1)[:, None]\n",
    "        s, _, _, _ = test_env.step(actions.repeat(1, test_env.n_players))\n",
    "    logits, values = model(s[0,0].to(device=DEVICE)[None, None, :])\n",
    "    logits = logits.cpu().numpy().squeeze()\n",
    "    values = values.cpu().numpy().squeeze()\n",
    "print()\n",
    "print(logits)\n",
    "print(values)\n",
    "print(s[:,0].squeeze().t())\n",
    "print(spearmanr(logits, test_env.thresholds.squeeze().cpu().numpy())[0],\n",
    "      pearsonr(logits, test_env.thresholds.squeeze().cpu().numpy())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_layer_idx = -1\n",
    "child_layer = list(model.children())[child_layer_idx]\n",
    "print(child_layer.transform_features.weight.data, child_layer.transform_features.bias.data)\n",
    "print(child_layer.recombine_features.weight.data, child_layer.recombine_features.bias.data)\n",
    "print(model)\n",
    "print(child_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_env = ve.KaggleMABEnvTorchVectorized(n_envs=3, n_players=2, decay_rate=0.97, env_device=torch.device('cpu'), out_device=DEVICE)\n",
    "test_env.reset()\n",
    "actions = torch.arange(6).view(3,2)\n",
    "test_env.step(actions)\n",
    "test_env.step(actions)\n",
    "actions = torch.arange(1,7).view(3,2)\n",
    "test_env.step(actions)\n",
    "test_env.step(actions)\n",
    "test_env.step(actions)\n",
    "\n",
    "test = test_env.player_n_pulls.unsqueeze(3).expand(-1,-1,-1,2).transpose(1,3)\n",
    "test = torch.cat([\n",
    "    test[:,0,:,:].unsqueeze(1),\n",
    "    test[:,0,:,[1,0]].unsqueeze(1)\n",
    "], dim=1)\n",
    "test[0,0]\n",
    "test_env.player_rewards_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated env rollouts with random policy\n",
    "test_env = ve.KaggleMABEnvTorchVectorized(n_envs=1000, n_players=2, decay_rate=0.97, env_device=torch.device('cuda'), out_device=torch.device('cuda'))\n",
    "\n",
    "for i in tqdm.trange(test_env.n_steps):\n",
    "    actions = torch.randint(test_env.n_bandits, size=(test_env.n_envs, test_env.n_players))\n",
    "    test_env.step(actions)\n",
    "print(f'Mean of player rewards: {test_env.player_rewards_sums.sum(axis=-1).mean():.2f}, Standard deviation of player rewards: {test_env.player_rewards_sums.sum(axis=-1).std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated env rollouts with omniscient optimal policy\n",
    "test_env = ve.KaggleMABEnvTorchVectorized(n_envs=1000, n_players=2, decay_rate=0.97, env_device=torch.device('cuda'), out_device=torch.device('cpu'))\n",
    "\n",
    "for i in tqdm.trange(test_env.n_steps):\n",
    "    actions = test_env.thresholds.argmax(axis=-1)[:, None]\n",
    "    test_env.step(actions.repeat(1, test_env.n_players))\n",
    "print(f'Mean of player rewards: {test_env.player_rewards_sums.sum(axis=-1).mean():.2f}, Standard deviation of player rewards: {test_env.player_rewards_sums.sum(axis=-1).std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 10\n",
    "activation_func = nn.ReLU()\n",
    "layer_size = 64\n",
    "layers = (\n",
    "    [FullyConnectedGNNLayer(n_nodes, 1, layer_size, activation_func=activation_func)] +\n",
    "    [FullyConnectedGNNLayer(n_nodes, layer_size, layer_size, activation_func=activation_func)] * 0 +\n",
    "    [FullyConnectedGNNLayer(n_nodes, layer_size, 1, activation_func=activation_func)]\n",
    ")\n",
    "model = nn.Sequential(\n",
    "    *layers\n",
    ")\n",
    "#model = FullyConnectedGNNLayer(n_nodes, 1, 1, activation_func=nn.Identity())\n",
    "model.train()\n",
    "model.to('cuda')\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0002)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for i in tqdm.trange(10):\n",
    "    train_batch_in = torch.rand((250, n_nodes, 1), dtype=torch.float).to('cuda')\n",
    "    k = 2\n",
    "    labels = torch.topk(train_batch_in, k=k, dim=1).indices[:,k-1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_batch_in)\n",
    "    loss = loss_func(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    accuracy = (F.softmax(outputs, dim=1).argmax(dim=1) == labels).float().mean()\n",
    "print(f'Loss: {loss:.2f}, Accuracy: {accuracy:.3f}')\n",
    "\n",
    "test_batch_in = torch.rand((1, n_nodes, 1), dtype=torch.float).to('cuda')\n",
    "print(test_batch_in)\n",
    "print(F.softmax(model(test_batch_in), dim=1))\n",
    "print(torch.topk(test_batch_in, k=k, dim=1).indices[:,k-1])\n",
    "print(F.softmax(model(test_batch_in), dim=1).argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.trange(1000):\n",
    "    train_batch_in = torch.rand((250, n_nodes, 1), dtype=torch.float).to('cuda')\n",
    "    k = 2\n",
    "    labels = torch.topk(train_batch_in, k=k, dim=1).indices[:,k-1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_batch_in)\n",
    "    loss = loss_func(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    accuracy = (F.softmax(outputs, dim=1).argmax(dim=1) == labels).float().mean()\n",
    "print(f'Loss: {loss:.2f}, Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedGNNLayer(5, 1, 1, activation_func=nn.Identity())\n",
    "model.train()\n",
    "model.to('cuda')\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.)\n",
    "\n",
    "for i in tqdm.trange(10000):\n",
    "    train_batch_in = torch.rand((1024, 5, 1), dtype=torch.float).to('cuda')\n",
    "    train_batch_out = train_batch_in + train_batch_in.sum(dim=1, keepdims=True)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_batch_in)\n",
    "    loss = loss_func(outputs, train_batch_out)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_in = torch.rand((2, 5, 1), dtype=torch.float).to('cuda')\n",
    "print(train_batch_in)\n",
    "print(train_batch_in + train_batch_in.sum(dim=1, keepdims=True))\n",
    "print(model(train_batch_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loosely based on: https://github.com/MorvanZhou/pytorch-A3C\n",
    "def a3c_vectorized_env(env, model, optimizer, n_episodes, batch_size,\n",
    "                       gamma=0.9):\n",
    "    model.to(device=DEVICE)\n",
    "    model.train()\n",
    "    episode_reward_sums = []\n",
    "    \n",
    "    for episode in tqdm.trange(n_episodes):\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        s, r, _, _ = env.reset()\n",
    "        episode_reward_sums.append(r)\n",
    "        step_count = 1\n",
    "        while not env.done:\n",
    "            # Batch size of 1 for inference\n",
    "            a = model.sample_action(s.to(device=DEVICE).unsqueeze(0)).squeeze(0)\n",
    "            next_s, r, done, _ = env.step(a)\n",
    "            \n",
    "            #print(s.shape, a.shape, r.shape)\n",
    "            buffer_s.append(s)\n",
    "            buffer_a.append(a)\n",
    "            buffer_r.append(r)\n",
    "            \n",
    "            if step_count % batch_size == 0 or done:\n",
    "                if done:\n",
    "                    v_next_s = torch.zeros_like(buffer_r[-1])\n",
    "                else:\n",
    "                    _, v_next_s = model(next_s.to(device=DEVICE).unsqueeze(0))\n",
    "                    v_next_s = v_next_s.detach().squeeze(0)\n",
    "                v_next_s.to(device=DEVICE)\n",
    "                \n",
    "                buffer_v_target = []\n",
    "                for r in buffer_r[::-1]:\n",
    "                    v_next_s = r + gamma * v_next_s\n",
    "                    buffer_v_target.append(v_next_s)\n",
    "                buffer_v_target.reverse()\n",
    "                \n",
    "                loss = model.loss_func(\n",
    "                    torch.stack(buffer_s).to(device=DEVICE),\n",
    "                    torch.stack(buffer_a).to(device=DEVICE),\n",
    "                    torch.stack(buffer_v_target).to(device=DEVICE)\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                buffer_s, buffer_a, buffer_r = [], [], []\n",
    "            s = next_s\n",
    "            episode_reward_sums[-1] += r\n",
    "            step_count += 1\n",
    "        episode_reward_sums[-1] = episode_reward_sums[-1].mean()\n",
    "    return episode_reward_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kaggle_MAB_Env_Numpy_Vectorized():\n",
    "    def __init__(self,\n",
    "                 # Kaggle MAB env params\n",
    "                 n_bandits=100, n_steps=1999, decay_rate=0.97, sample_resolution=100,\n",
    "                 # Custom params\n",
    "                 n_envs=1, n_players=2, reward_type=EVERY_STEP, remove_extra_dims=False):\n",
    "        # Assert parameter conditions\n",
    "        assert 0 <= decay_rate <= 1.\n",
    "        assert reward_type in (END_OF_GAME, EVERY_STEP)\n",
    "        if reward_type == END_OF_GAME:\n",
    "            assert n_players >= 2\n",
    "        else:\n",
    "            assert n_players >= 1\n",
    "        self.n_bandits = n_bandits\n",
    "        self.n_steps = n_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.sample_resolution = sample_resolution\n",
    "        \n",
    "        self.n_envs = n_envs\n",
    "        self.n_players = n_players\n",
    "        self.reward_type = reward_type\n",
    "        self.remove_extra_dims = remove_extra_dims\n",
    "        \n",
    "        self.timestep = None\n",
    "        self.orig_thresholds = None\n",
    "        self.player_n_pulls = None\n",
    "        self.player_rewards_sums = None\n",
    "        self.reset()\n",
    "    \n",
    "    @property\n",
    "    def thresholds(self):\n",
    "        return self.orig_thresholds * (self.decay_rate ** self.player_n_pulls[:,0,:])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.orig_thresholds = np.random.randint(self.sample_resolution + 1, size=(self.n_envs, self.n_bandits)).astype(np.float32)\n",
    "        self.player_n_pulls = np.zeros((self.n_envs, self.n_players, self.n_bandits))\n",
    "        self.player_rewards_sums = np.zeros_like(self.player_n_pulls)\n",
    "        \n",
    "        rewards = np.zeros((self.n_envs, self.n_players))\n",
    "        if self.remove_extra_dims:\n",
    "            return self.get_obs().squeeze(), rewards.squeeze(), self.done, dict(thresholds=self.thresholds.squeeze())\n",
    "        else:\n",
    "            return self.get_obs(), rewards, self.done, dict(thresholds=self.thresholds)\n",
    "        \n",
    "    def step(self, actions):\n",
    "        if self.remove_extra_dims:\n",
    "            if type(actions) == int:\n",
    "                actions = np.array([[actions]])\n",
    "            else:\n",
    "                actions = actions.reshape((self.n_envs, self.n_players))\n",
    "        assert actions.shape == (self.n_envs, self.n_players), f'actions.shape was: {actions.shape}'\n",
    "        assert not self.done\n",
    "        self.timestep += 1\n",
    "        \n",
    "        # Compute agent rewards\n",
    "        selected_thresholds = np.take_along_axis(self.thresholds, actions, axis=1)\n",
    "        pull_rewards = np.random.randint(self.sample_resolution, size=selected_thresholds.shape) < selected_thresholds\n",
    "        #pull_rewards = selected_thresholds / float(self.sample_resolution)\n",
    "        \n",
    "        # Update player_n_pulls and player_rewards_sums\n",
    "        for env_idx, env_actions in enumerate(actions):\n",
    "            for player_idx, pull_idx in enumerate(env_actions):\n",
    "                self.player_n_pulls[env_idx, :, pull_idx] += 1\n",
    "                self.player_rewards_sums[env_idx, player_idx, pull_idx] += pull_rewards[env_idx, player_idx]\n",
    "        \n",
    "        # Return (obs, reward, done) tuple\n",
    "        if self.reward_type == END_OF_GAME:\n",
    "            rewards = np.zeros_like(actions)\n",
    "            if self.timestep == self.n_steps:\n",
    "                rewards_sums = self.player_rewards_sums.sum(axis=2)\n",
    "                winners = rewards_sums.argmax(axis=1)\n",
    "                rewards[winners] = 1\n",
    "        elif self.reward_type == EVERY_STEP:\n",
    "            rewards = pull_rewards\n",
    "        \n",
    "        # State, reward, done, info_dict\n",
    "        if self.remove_extra_dims:\n",
    "            return self.get_obs().squeeze(), rewards.squeeze(), self.done, dict(thresholds=self.thresholds.squeeze())\n",
    "        else:\n",
    "            return self.get_obs(), rewards, self.done, dict(thresholds=self.thresholds)\n",
    "    \n",
    "    def get_obs(self):\n",
    "        # TODO: Clearly do not return thresholds or orig_thresholds\n",
    "        return np.stack([\n",
    "            #np.broadcast_to(self.orig_thresholds[:, None, :] / self.sample_resolution, self.player_n_pulls.shape),\n",
    "            self.player_n_pulls,\n",
    "            self.player_rewards_sums\n",
    "        ], axis=-1).astype(np.float32)\n",
    "    \n",
    "    @property\n",
    "    def done(self):\n",
    "        return self.timestep >= self.n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN_A3C(nn.Module):\n",
    "    def __init__(self, in_features, n_bandits, activation_func=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.activation_func = activation_func\n",
    "        layer_size = 128\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(self.in_features, layer_size),\n",
    "            activation_func,\n",
    "            nn.Linear(layer_size, layer_size),\n",
    "            activation_func,\n",
    "        )\n",
    "        self.actor = nn.Linear(layer_size, n_bandits)\n",
    "        self.critic = nn.Linear(layer_size, 1)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        in_shape = states.shape\n",
    "        base_out = self.base(states.view(-1, self.in_features))\n",
    "        return self.activation_func(self.actor(base_out)).view(in_shape[:-1]), self.critic(base_out).view(in_shape[:-2])\n",
    "    \n",
    "    def sample_action(self, states):\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.forward(states)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            batch_size, n_envs, n_players, n_bandits = probs.shape\n",
    "            m = distributions.Categorical(probs.view(batch_size * n_envs * n_players, n_bandits))\n",
    "            return m.sample().view(batch_size, n_envs, n_players)\n",
    "    \n",
    "    def choose_best_action(self, states):\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.forward(states)\n",
    "            return logits.argmax(dim=-1)\n",
    "        \n",
    "    def loss_func(self, states, actions, v_t):\n",
    "        #print(f'states.shape: {states.shape}, actions.shape: {actions.shape}, v_t.shape: {v_t.shape}')\n",
    "        logits, values = self.forward(states)\n",
    "        \n",
    "        #print(f'logits.shape: {logits.shape}, values.shape: {values.shape}')\n",
    "        td = v_t - values\n",
    "        critic_loss = td.pow(2).view(-1)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        batch_size, n_envs, n_players, n_bandits = probs.shape\n",
    "        m = distributions.Categorical(probs.view(batch_size * n_envs * n_players, n_bandits))\n",
    "        #print(f'm.log_prob(actions.view(batch_size * n_envs * n_players)).shape: {m.log_prob(actions.view(batch_size * n_envs * n_players)).shape}, td.shape: {td.shape}')\n",
    "        actor_loss = -(m.log_prob(actions.view(-1)) * td.detach().view(-1))\n",
    "        total_loss = (critic_loss + actor_loss).mean()\n",
    "        return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
